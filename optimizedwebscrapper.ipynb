{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run selenium and chrome driver to scrape data from cloudbytes.dev\n",
    "import time\n",
    "import os.path\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.headless = True  # Ensure GUI is off\n",
    "\n",
    "# Set path to chromedriver as per your configuration\n",
    "homedir = os.path.expanduser(\"~\")\n",
    "webdriver_service = Service(f\"{homedir}/ao3lockwood-co/chromedriver\")\n",
    "\n",
    "# Choose Chrome Browser\n",
    "browser = webdriver.Chrome(service=webdriver_service, options=chrome_options)\n",
    "\n",
    "# Get page\n",
    "pagenum = 1\n",
    "link = f\"https://archiveofourown.org/tags/Lockwood%20*a*%20Co*d*%20-%20Jonathan%20Stroud/works?page={pagenum}\"\n",
    "browser.get(link)\n",
    "\n",
    "maxpagenum = int(browser.find_element(By.XPATH, '//ol[1]/li[13]').text.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date and time = 11052023_1621\n",
      "Page 1 has been processed\n",
      "Processing page 2/74\n",
      "40\n",
      "Processing page 3/74\n",
      "60\n",
      "Processing page 4/74\n",
      "80\n",
      "Processing page 5/74\n",
      "100\n",
      "Processing page 6/74\n",
      "120\n",
      "Processing page 7/74\n",
      "140\n",
      "Processing page 8/74\n",
      "160\n",
      "Processing page 9/74\n",
      "180\n",
      "Processing page 10/74\n",
      "200\n",
      "Processing page 11/74\n",
      "220\n",
      "Processing page 12/74\n",
      "240\n",
      "Processing page 13/74\n",
      "260\n",
      "Processing page 14/74\n",
      "280\n",
      "Processing page 15/74\n",
      "300\n",
      "Processing page 16/74\n",
      "320\n",
      "Processing page 17/74\n",
      "340\n",
      "Processing page 18/74\n",
      "360\n",
      "Processing page 19/74\n",
      "380\n",
      "Processing page 20/74\n",
      "400\n",
      "Processing page 21/74\n",
      "420\n",
      "Processing page 22/74\n",
      "440\n",
      "Processing page 23/74\n",
      "460\n",
      "Processing page 24/74\n",
      "480\n",
      "Processing page 25/74\n",
      "500\n",
      "Processing page 26/74\n",
      "520\n",
      "Processing page 27/74\n",
      "540\n",
      "Processing page 28/74\n",
      "560\n",
      "Processing page 29/74\n",
      "580\n",
      "Processing page 30/74\n",
      "600\n",
      "Processing page 31/74\n",
      "620\n",
      "Processing page 32/74\n",
      "640\n",
      "Processing page 33/74\n",
      "660\n",
      "Processing page 34/74\n",
      "680\n",
      "Processing page 35/74\n",
      "700\n",
      "Processing page 36/74\n",
      "720\n",
      "Processing page 37/74\n",
      "740\n",
      "Processing page 38/74\n",
      "760\n",
      "Processing page 39/74\n",
      "780\n",
      "Processing page 40/74\n",
      "800\n",
      "Processing page 41/74\n",
      "820\n",
      "Processing page 42/74\n",
      "840\n",
      "Processing page 43/74\n",
      "860\n",
      "Processing page 44/74\n",
      "880\n",
      "Processing page 45/74\n",
      "900\n",
      "Processing page 46/74\n",
      "920\n",
      "Processing page 47/74\n",
      "940\n",
      "Processing page 48/74\n",
      "960\n",
      "Processing page 49/74\n",
      "980\n",
      "Processing page 50/74\n",
      "1000\n",
      "Processing page 51/74\n",
      "1020\n",
      "Processing page 52/74\n",
      "1040\n",
      "Processing page 53/74\n",
      "1060\n",
      "Processing page 54/74\n",
      "1080\n",
      "Processing page 55/74\n",
      "1100\n",
      "Processing page 56/74\n",
      "1120\n",
      "Processing page 57/74\n",
      "1140\n",
      "Processing page 58/74\n",
      "1160\n",
      "Processing page 59/74\n",
      "1180\n",
      "Processing page 60/74\n",
      "1200\n",
      "Processing page 61/74\n",
      "1220\n",
      "Processing page 62/74\n",
      "1240\n",
      "Processing page 63/74\n",
      "1260\n",
      "Processing page 64/74\n",
      "1280\n",
      "Processing page 65/74\n",
      "1300\n",
      "Processing page 66/74\n",
      "1320\n",
      "Processing page 67/74\n",
      "1340\n",
      "Processing page 68/74\n",
      "1360\n",
      "Processing page 69/74\n",
      "1380\n",
      "Processing page 70/74\n",
      "1400\n",
      "Processing page 71/74\n",
      "1420\n",
      "Processing page 72/74\n",
      "1440\n",
      "Processing page 73/74\n",
      "1460\n",
      "Processing page 74/74\n",
      "1473\n"
     ]
    }
   ],
   "source": [
    "def get_links(browser):\n",
    "    works = browser.find_elements(By.XPATH, '//ol[2]/li')\n",
    "    # Iterate through each work and extract author and datetime\n",
    "    data=[]\n",
    "    for work in works:\n",
    "        h4 = work.find_element(By.TAG_NAME,'h4')\n",
    "        a = h4.find_elements(By.TAG_NAME, 'a')\n",
    "        links = a[0].get_attribute(\"href\")\n",
    "        data.append(links)\n",
    "    return data\n",
    "def process_pages(browser, maxpagenum):\n",
    "    # datetime object containing current date and time\n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"%d%m%Y_%H%M\")\n",
    "    print(\"date and time =\", dt_string)\n",
    "\n",
    "    # Create an empty list to hold the data\n",
    "    data_list = get_links(browser)\n",
    "    print('Page 1 has been processed')\n",
    "\n",
    "    # Iterate through each page and append the data to the list\n",
    "    for p in range(2, maxpagenum + 1):\n",
    "        pagenum = p\n",
    "        time.sleep(10)\n",
    "        print(f'Processing page {pagenum}/{maxpagenum}')\n",
    "        link = \"https://archiveofourown.org/tags/Lockwood%20*a*%20Co*d*%20-%20Jonathan%20Stroud/works?page=\" + str(pagenum)\n",
    "        browser.get(link)\n",
    "        data_list += get_links(browser)\n",
    "        print(len(data_list))\n",
    "\n",
    "    return data_list\n",
    "\n",
    "links_list = process_pages(browser, maxpagenum)\n",
    "# Wait for 10 seconds\n",
    "time.sleep(10)\n",
    "browser.quit()\n",
    "link=pd.DataFrame(links_list, columns=['links'])\n",
    "link.to_csv('links.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting missing data 1/1473\n",
      "getting missing data 2/1473\n",
      "getting missing data 3/1473\n",
      "getting missing data 4/1473\n",
      "getting missing data 5/1473\n",
      "getting missing data 6/1473\n",
      "getting missing data 7/1473\n",
      "getting missing data 8/1473\n",
      "getting missing data 9/1473\n",
      "getting missing data 10/1473\n",
      "getting missing data 11/1473\n",
      "getting missing data 12/1473\n",
      "getting missing data 13/1473\n",
      "getting missing data 14/1473\n",
      "getting missing data 15/1473\n",
      "getting missing data 16/1473\n",
      "getting missing data 17/1473\n",
      "getting missing data 18/1473\n",
      "getting missing data 19/1473\n",
      "getting missing data 20/1473\n",
      "getting missing data 21/1473\n",
      "getting missing data 22/1473\n",
      "getting missing data 23/1473\n",
      "getting missing data 24/1473\n",
      "getting missing data 25/1473\n",
      "getting missing data 26/1473\n",
      "getting missing data 27/1473\n",
      "getting missing data 28/1473\n",
      "getting missing data 29/1473\n",
      "getting missing data 30/1473\n",
      "getting missing data 31/1473\n",
      "getting missing data 32/1473\n",
      "getting missing data 33/1473\n",
      "getting missing data 34/1473\n",
      "getting missing data 35/1473\n",
      "getting missing data 36/1473\n",
      "getting missing data 37/1473\n",
      "getting missing data 38/1473\n",
      "getting missing data 39/1473\n",
      "getting missing data 40/1473\n",
      "getting missing data 41/1473\n",
      "getting missing data 42/1473\n",
      "getting missing data 43/1473\n",
      "getting missing data 44/1473\n",
      "getting missing data 45/1473\n",
      "getting missing data 46/1473\n",
      "getting missing data 47/1473\n",
      "getting missing data 48/1473\n",
      "getting missing data 49/1473\n",
      "getting missing data 50/1473\n",
      "getting missing data 51/1473\n",
      "getting missing data 52/1473\n",
      "getting missing data 53/1473\n",
      "getting missing data 54/1473\n",
      "getting missing data 55/1473\n",
      "getting missing data 56/1473\n",
      "getting missing data 57/1473\n",
      "getting missing data 58/1473\n",
      "getting missing data 59/1473\n",
      "getting missing data 60/1473\n",
      "getting missing data 61/1473\n",
      "getting missing data 62/1473\n",
      "getting missing data 63/1473\n",
      "getting missing data 64/1473\n",
      "getting missing data 65/1473\n",
      "getting missing data 66/1473\n",
      "getting missing data 67/1473\n",
      "getting missing data 68/1473\n",
      "getting missing data 69/1473\n",
      "getting missing data 70/1473\n",
      "getting missing data 71/1473\n",
      "getting missing data 72/1473\n",
      "getting missing data 73/1473\n",
      "getting missing data 74/1473\n",
      "getting missing data 75/1473\n",
      "getting missing data 76/1473\n",
      "getting missing data 77/1473\n",
      "getting missing data 78/1473\n",
      "getting missing data 79/1473\n",
      "getting missing data 80/1473\n",
      "getting missing data 81/1473\n",
      "getting missing data 82/1473\n",
      "getting missing data 83/1473\n",
      "getting missing data 84/1473\n",
      "getting missing data 85/1473\n",
      "getting missing data 86/1473\n",
      "getting missing data 87/1473\n",
      "getting missing data 88/1473\n",
      "getting missing data 89/1473\n",
      "getting missing data 90/1473\n",
      "getting missing data 91/1473\n",
      "getting missing data 92/1473\n",
      "getting missing data 93/1473\n",
      "getting missing data 94/1473\n",
      "getting missing data 95/1473\n",
      "getting missing data 96/1473\n",
      "getting missing data 97/1473\n",
      "getting missing data 98/1473\n",
      "getting missing data 99/1473\n",
      "getting missing data 100/1473\n",
      "getting missing data 101/1473\n",
      "getting missing data 102/1473\n",
      "getting missing data 103/1473\n",
      "getting missing data 104/1473\n",
      "getting missing data 105/1473\n",
      "getting missing data 106/1473\n",
      "getting missing data 107/1473\n",
      "getting missing data 108/1473\n",
      "getting missing data 109/1473\n",
      "getting missing data 110/1473\n",
      "getting missing data 111/1473\n",
      "getting missing data 112/1473\n",
      "getting missing data 113/1473\n",
      "getting missing data 114/1473\n",
      "getting missing data 115/1473\n",
      "getting missing data 116/1473\n",
      "getting missing data 117/1473\n",
      "getting missing data 118/1473\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6156/2358790928.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0mfinal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinks_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_6156/2358790928.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(links_list)\u001b[0m\n\u001b[1;32m     55\u001b[0m         }\n\u001b[1;32m     56\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslow_links\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_data(links_list):\n",
    "    data = []\n",
    "    slow_links = []  # List to store links that are taking too long to access\n",
    "\n",
    "    for x in links_list:\n",
    "        newlink = f\"{x}?view_adult=true\"\n",
    "        print(f\"getting missing data {links_list.index(x)+1}/{len(links_list)}\")\n",
    "        try:\n",
    "            source = requests.get(newlink, headers={'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64)'}).text\n",
    "        except requests.exceptions.RequestException:\n",
    "            print(f\"Link {x} is taking too long to access. Adding to slow_links list.\")\n",
    "            slow_links.append(x)\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(source, 'html.parser')\n",
    "        title = soup.find('h2', attrs={'class': 'title heading'}).get_text().replace('\\n', '').strip()\n",
    "        author = soup.find('a', attrs={'rel': 'author'}).get_text() if soup.find('a', attrs={'rel': 'author'}) else \"Anonymous\"\n",
    "        datetime = soup.find('dd', attrs={'class': 'published'}).get_text()\n",
    "        updated = soup.find('dd', attrs={'class': 'status'}).get_text() if soup.find('dd', attrs={'class': 'status'}) else datetime\n",
    "        chapters = soup.find('dd', attrs={'class': 'chapters'}).get_text()\n",
    "        language = soup.find('dd', attrs={'class': 'language'}).get_text().replace('\\n', '').strip()\n",
    "        words = soup.find('dd', attrs={'class': 'words'}).get_text()\n",
    "        kudos = soup.find('dd', attrs={'class': 'kudos'}).get_text() if soup.find('dd', attrs={'class': 'kudos'}) else 0\n",
    "        comments = soup.find('dd', attrs={'class': 'comments'}).get_text() if soup.find('dd', attrs={'class': 'comments'}) else 0\n",
    "        bookmarks = soup.find('dd', attrs={'class': 'bookmarks'}).get_text() if soup.find('dd', attrs={'class': 'bookmarks'}) else 0\n",
    "        hits = soup.find('dd', attrs={'class': 'hits'}).get_text() if soup.find('dd', attrs={'class': 'hits'}) else 0\n",
    "        warning = soup.find('dd', attrs={'class': 'warning tags'}).get_text().replace('\\n', '').strip()\n",
    "        ships_list = [r.get_text().strip() for r in soup.find_all('dd', attrs={'class': 'relationship tags'})]\n",
    "        mainrelationship = ships_list[0] if ships_list else 'None'\n",
    "        character_list = [c.get_text().strip() for c in soup.find_all('dd', attrs={'class': 'character tags'})]\n",
    "        tags_list = [t.get_text().strip() for t in soup.find_all('dd', attrs={'class': 'freeform tags'})]\n",
    "        position_list = [p.get_text().strip() for p in soup.find('span', attrs={'class': 'position'}).find('a')] if soup.find('span', attrs={'class': 'position'}) else 'not a series'\n",
    "        summary = soup.find('div', attrs={'class': 'summary module'}).get_text().replace('\\n', ' ').replace('Summary:', '').strip() if soup.find('div', attrs={'class': 'summary module'}) else np.nan\n",
    "        rating_tag = soup.find('dd', attrs={'class': 'rating tags'}).get_text().replace('\\n', '').strip()\n",
    "        row = {\n",
    "            'link': x,\n",
    "            'title': title,\n",
    "            'author': author,\n",
    "            'published': datetime,\n",
    "            'updatedate': updated,\n",
    "            'chapters': chapters,\n",
    "            'language': language,\n",
    "            'words': words,\n",
    "            'kudos': kudos,\n",
    "            'comments': comments,\n",
    "            'bookmarks': bookmarks,\n",
    "            'hits': hits,\n",
    "            'warning': warning,\n",
    "            'mainship': mainrelationship,\n",
    "            'relationship': ships_list,\n",
    "            'characters': character_list,\n",
    "            'tags': tags_list,\n",
    "            'summary': summary,\n",
    "            'rating': rating_tag\n",
    "        }\n",
    "        data.append(row)\n",
    "        time.sleep(10)\n",
    "\n",
    "    for l in slow_links:\n",
    "        print(l)\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "final = get_data(links_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the chapter column into chapter and chapter_max, and create a completion column\n",
    "final[['chapter','chapter_max']] = final.chapters.str.split(\"/\", expand=True)\n",
    "final['completion'] = final.apply(lambda row: 'completed' if row['chapter']==row['chapter_max'] else 'incomplete', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=f'ao3_lockwood_and_co_ao_11052023_1335.csv'\n",
    "final.to_csv(filename, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimal_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
