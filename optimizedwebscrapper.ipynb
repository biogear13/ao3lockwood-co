{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages: 72\n",
      "date and time = 05052023_1100\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11930/1498081302.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_11930/1498081302.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mThreadPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mlink_pages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_links\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_page_num\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0mlinks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlink\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlink_pages\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'links collected'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/minimal_ds/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    606\u001b[0m                     \u001b[0;31m# Careful not to keep a reference to the popped future\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m                         \u001b[0;32myield\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/minimal_ds/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/minimal_ds/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BASE_URL = \"https://archiveofourown.org/tags/Lockwood%20*a*%20Co*d*%20-%20Jonathan%20Stroud/works?page=\"\n",
    "\n",
    "# Setup chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.headless = True # Ensure GUI is off\n",
    "\n",
    "homedir = os.path.expanduser(\"~\")\n",
    "webdriver_service = Service(f\"{homedir}/ao3lockwood-co/chromedriver\")\n",
    "\n",
    "def get_links(page_number):\n",
    "    browser = webdriver.Chrome(service=webdriver_service, options=chrome_options)\n",
    "    link = BASE_URL+str(page_number)\n",
    "    browser.get(link)\n",
    "    print('hello')\n",
    "    time.sleep(10)  # ensure page is loaded\n",
    "    works = browser.find_elements(By.XPATH, '//ol[2]/li')\n",
    "    data = [work.find_element(By.TAG_NAME,'h4').find_elements(By.TAG_NAME, 'a')[0].get_attribute(\"href\") for work in works]\n",
    "    browser.quit()\n",
    "    return data\n",
    "\n",
    "def get_data(link):\n",
    "    headers={'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64)'}\n",
    "    link += '?view_adult=true'\n",
    "    # rest of your code to get the data from the link...\n",
    "    try:\n",
    "        source = requests.get(link, headers=headers).text\n",
    "        soup = BeautifulSoup(source, 'html.parser')\n",
    "    except requests.exceptions.RequestException:\n",
    "        print(f\"Link {link} is taking too long to access. Adding to slow_links list.\")\n",
    "        return None\n",
    "\n",
    "    attrs_map = {\n",
    "        'title': ('h2', {'class':'title heading'}),\n",
    "        'author': ('a', {'rel':'author'}),\n",
    "        'published': ('dd', {'class':'published'}),\n",
    "        'updatedate': ('dd', {'class':'status'}),\n",
    "        'chapters': ('dd', {'class':'chapters'}),\n",
    "        'language': ('dd', {'class':'language'}),\n",
    "        'words': ('dd', {'class':'words'}),\n",
    "        'kudos': ('dd', {'class':'kudos'}),\n",
    "        'comments': ('dd', {'class':'comments'}),\n",
    "        'bookmarks': ('dd', {'class':'bookmarks'}),\n",
    "        'hits': ('dd', {'class':'hits'}),\n",
    "        'warning': ('dd', {'class':'warning tags'}),\n",
    "        'summary': ('div', {'class':'summary module'}),\n",
    "        'rating': ('dd', {'class':'rating tags'}),\n",
    "    }\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    for key, value in attrs_map.items():\n",
    "        try:\n",
    "            if key in [\"author\", \"updatedate\", \"summary\"]:\n",
    "                data[key] = soup.find(value[0], value[1]).get_text().strip()\n",
    "            else:\n",
    "                data[key] = soup.find(value[0], value[1]).get_text().replace('\\n','').strip()\n",
    "        except AttributeError:\n",
    "            if key == \"author\":\n",
    "                data[key] = \"Anonymous\"\n",
    "            elif key == \"updatedate\":\n",
    "                data[key] = data[\"published\"]\n",
    "            elif key == \"summary\":\n",
    "                data[key] = np.nan\n",
    "            else:\n",
    "                data[key] = 0\n",
    "    \n",
    "    data['mainship'], data['relationship'] = extract_relationships(soup)\n",
    "    data['characters'] = extract_tags(soup, 'character tags')\n",
    "    data['tags'] = extract_tags(soup, 'freeform tags')\n",
    "    data['series'] = extract_series(soup)\n",
    "\n",
    "    return data\n",
    "\n",
    "def extract_relationships(soup):\n",
    "    try:\n",
    "        ships = soup.find('dd', attrs={'class':'relationship tags'})\n",
    "        ships_list = ships.find_all('a', attrs={'class':'tag'})\n",
    "        mainrelationship = ships_list[0].get_text().strip()\n",
    "        relationship_list = [r.get_text().strip() for r in ships_list]\n",
    "    except:\n",
    "        mainrelationship='None'\n",
    "        relationship_list = []\n",
    "    return mainrelationship, relationship_list\n",
    "\n",
    "def extract_tags(soup, class_name):\n",
    "    try:\n",
    "        tags_section = soup.find('dd', attrs={'class': class_name})\n",
    "        tags_list = tags_section.find_all('a', attrs={'class':'tag'})\n",
    "        tags = [t.get_text().strip() for t in tags_list]\n",
    "    except:\n",
    "        tags = []\n",
    "    return tags\n",
    "\n",
    "def extract_series(soup):\n",
    "    try:\n",
    "        position = soup.find('span', attrs={'class':'position'})\n",
    "        position_list = position.find_all('a')\n",
    "        series_list = [p.get_text().strip() for p in position_list]\n",
    "    except:\n",
    "        series_list = []\n",
    "    return series_list\n",
    "\n",
    "def main():\n",
    "    browser = webdriver.Chrome(service=webdriver_service, options=chrome_options)\n",
    "    browser.get(BASE_URL+'1')\n",
    "    max_page_num = int(browser.find_element(By.XPATH,'//ol[1]/li[13]').text.strip())\n",
    "    browser.quit()\n",
    "\n",
    "    print(f'Total pages: {max_page_num}')\n",
    "\n",
    "    # datetime object containing current date and time\n",
    "    now = datetime.now()\n",
    "\n",
    "    # dd/mm/YY H:M:S\n",
    "    dt_string = now.strftime(\"%d%m%Y_%H%M\")\n",
    "    print(\"date and time =\", dt_string)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=1) as executor:\n",
    "        link_pages = list(executor.map(get_links, range(1, max_page_num + 1)))\n",
    "    links = [link for page in link_pages for link in page]\n",
    "    print('links collected')\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=1) as executor:\n",
    "        data = list(executor.map(get_data, links))\n",
    "    print('data collected')\n",
    "\n",
    "    # rest of your code to process and save the data...\n",
    "    final = pd.DataFrame(data)\n",
    "    # Split the chapter column into chapter and chapter_max, and create a completion column\n",
    "    final[['chapter','chapter_max']] = final.chapters.str.split(\"/\", expand=True)\n",
    "    final['completion'] = final.apply(lambda row: 'completed' if row['chapter']==row['chapter_max'] else 'incomplete', axis=1)\n",
    "    filename=f'ao3_lockwood_and_co_ao_{dt_string}.csv'\n",
    "    final.to_csv(filename, index=False)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_items(column):\n",
    "    items = {}\n",
    "    for row in column:\n",
    "        for item in row:\n",
    "            item = item.strip()\n",
    "            items[item] = items.get(item, 0) + 1\n",
    "    return {k: items[k] for k in sorted(items)}\n",
    "\n",
    "def get_item_count(column):\n",
    "    return [len(row) if row != [''] else 0 for row in column]\n",
    "\n",
    "def get_df_item(title_column, item_column, name_col):\n",
    "    item_list = []\n",
    "    for title, row_items in zip(title_column, item_column):\n",
    "        for item in row_items:\n",
    "            item = item.strip()\n",
    "            if '&' not in item:\n",
    "                item_list.append([title, item])\n",
    "    return pd.DataFrame(item_list, columns=['title', name_col])\n",
    "\n",
    "def process_dataframe(filename):\n",
    "    df = pd.read_csv(filename, converters={'characters': eval, 'relationship': eval, 'tags': eval})\n",
    "    df['published'] = pd.to_datetime(df['published'])\n",
    "    df['updatedate'] = pd.to_datetime(df['updatedate'])\n",
    "    current_date = df['updatedate'].max()\n",
    "    df['datediff_pub'] = (current_date - df['published']).dt.days\n",
    "    df['datediff'] = (current_date - df['updatedate']).dt.days\n",
    "    df['classification'] = df.apply(lambda row: 'oneshot' if row['chapter_max'] == '1' else ('multichapter(complete)' if row['completion'] == 'completed' else ('multichapter(updating)' if row['datediff'] <= 60 else 'multichapter(dormant)')), axis=1)\n",
    "\n",
    "    author_df = df.groupby(['author'], as_index=False).agg({'updatedate': 'max', 'published': 'min'}).rename(columns={'updatedate': 'lastauthorupdate', 'published': 'firstauthorupdate'})\n",
    "    df = df.merge(author_df, how='left', on='author')\n",
    "    df['author_lastupdate_diff'] = (current_date - df['lastauthorupdate']).dt.days\n",
    "    df['daysactive'] = (df['lastauthorupdate'] - df['firstauthorupdate']).dt.days\n",
    "    df['daysincefirtupload'] = (current_date - df['firstauthorupdate']).dt.days\n",
    "    df['author_activity'] = df['author_lastupdate_diff'].apply(lambda x: 'active' if x <= 60 else 'inactive')\n",
    "\n",
    "    df['num_relationship'] = get_item_count(df['relationship'])\n",
    "    df['num_characters'] = get_item_count(df['characters'])\n",
    "    df['num_tags'] = get_item_count(df['tags'])\n",
    "\n",
    "    char_df = get_df_item(df['title'], df['characters'], 'charactername')\n",
    "    relationship_df = get_df_item(df['title'], df['relationship'], 'shiptag')\n",
    "    tags_df = get_df_item(df['title'], df['tags'], 'tag_item')\n",
    "\n",
    "    char_df.to_csv('character_relationship_tags.csv', index=False)\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    process_dataframe('input.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimal_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
