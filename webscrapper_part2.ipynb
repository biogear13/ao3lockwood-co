{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run selenium and chrome driver to scrape data from cloudbytes.dev\n",
    "import time\n",
    "import os.path\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.headless = True # Ensure GUI is off\n",
    "chrome_options.add_argument(\"--window-size=1920,1200\")\n",
    "\n",
    "# Set path to chromedriver as per your configuration\n",
    "homedir = os.path.expanduser(\"~\")\n",
    "webdriver_service = Service(f\"{homedir}/ao3lockwood-co/chromedriver\")\n",
    "\n",
    "# Choose Chrome Browser\n",
    "browser = webdriver.Chrome(service=webdriver_service, options=chrome_options)\n",
    "\n",
    "# Get page\n",
    "pagenum=1\n",
    "link=\"https://archiveofourown.org/tags/Lockwood%20*a*%20Co*d*%20-%20Jonathan%20Stroud/works?page=\"+str(pagenum)\n",
    "browser.get(link)\n",
    "\n",
    "# Get the total number of pages to scrape\n",
    "maxpagenum=int(browser.find_element(By.XPATH,'//ol[1]/li[13]').text.strip())\n",
    "\n",
    "def get_title(browser):\n",
    "    \"\"\"\n",
    "    Extracts data from each work listed on a page.\n",
    "    :param browser: Chrome Webdriver object\n",
    "    :return: Pandas DataFrame\n",
    "    \"\"\"\n",
    "    # Find all the fanfic works on the page\n",
    "    works = browser.find_elements(By.XPATH, '//ol[2]/li')\n",
    "    # Iterate through each work and extract author and datetime\n",
    "    data=[]\n",
    "    for work in works:\n",
    "        WebDriverWait(browser, 10)\n",
    "        h4 = work.find_element(By.TAG_NAME,'h4')\n",
    "        a = h4.find_elements(By.TAG_NAME, 'a')\n",
    "        links = a[0].get_attribute(\"href\")\n",
    "        language = work.find_elements(By.CLASS_NAME,'language')[1].text.strip()\n",
    "        words = work.find_elements(By.CLASS_NAME,'words')[1].text.strip()\n",
    "        try:\n",
    "            kudos = work.find_elements(By.CLASS_NAME,'kudos')[1].text.strip()\n",
    "        except:\n",
    "            kudos = 0\n",
    "        try:\n",
    "            hits = work.find_elements(By.CLASS_NAME,'hits')[1].text.strip()\n",
    "        except:\n",
    "            hits = 0\n",
    "        try:\n",
    "            comments = work.find_elements(By.CLASS_NAME,'comments')[1].text.strip()\n",
    "        except:\n",
    "            comments = 0\n",
    "        try:\n",
    "            bookmarks = work.find_elements(By.CLASS_NAME,'bookmarks')[1].text.strip()\n",
    "        except:\n",
    "            bookmarks = 0\n",
    "        row = {'link':links, \n",
    "               'language':language,\n",
    "               'words':words,\n",
    "               'kudos':kudos, \n",
    "               'comments':comments,\n",
    "               'bookmarks':bookmarks,\n",
    "               'hits':hits\n",
    "               }\n",
    "        data.append(row)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Get data from the first page\n",
    "data_list=get_title(browser)\n",
    "\n",
    "# Iterate through remaining pages\n",
    "for p in range(2,maxpagenum+1):\n",
    "    pagenum=p\n",
    "    time.sleep(10)\n",
    "    print(f'procesing page {pagenum}')\n",
    "    link=\"https://archiveofourown.org/tags/Lockwood%20*a*%20Co*d*%20-%20Jonathan%20Stroud/works?page=\"+str(pagenum)\n",
    "    browser.get(link)\n",
    "    data_list=data_list.append(get_title(browser), ignore_index=True)\n",
    "\n",
    "# Wait for 10 seconds\n",
    "time.sleep(10)\n",
    "browser.quit()\n",
    "\n",
    "# Remove empty cells and create a new DataFrame with missing data\n",
    "data_list1=data_list.replace('', np.nan)\n",
    "data_list1=data_list1[data_list1.isna().any(axis=1)]\n",
    "data_list2 = data_list[~data_list['link'].isin(data_list1['link'])]\n",
    "\n",
    "def get_missing(df):\n",
    "    \"\"\"\n",
    "    Extracts data for works that had missing cells in the original DataFrame.\n",
    "    :param df: Pandas DataFrame\n",
    "    :return: Pandas DataFrame\n",
    "    \"\"\"\n",
    "    data=[]\n",
    "    for x in df['link']:\n",
    "        newlink=x+'?view_adult=true'\n",
    "        print(newlink)\n",
    "        source = requests.get(newlink, headers={\n",
    "                          'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64)'}).text\n",
    "        soup = BeautifulSoup(source,'html.parser')\n",
    "        language=soup.find('dd', attrs={'class':'language'}).get_text().replace('\\n','').strip()\n",
    "        words=soup.find('dd', attrs={'class':'words'}).get_text()\n",
    "        try:\n",
    "            kudos=soup.find('dd', attrs={'class':'kudos'}).get_text()\n",
    "        except:\n",
    "            kudos=0\n",
    "        try:\n",
    "            comments=soup.find('dd', attrs={'class':'comments'}).get_text()\n",
    "        except:\n",
    "            comments=0\n",
    "        try:\n",
    "            bookmarks=soup.find('dd', attrs={'class':'bookmarks'}).get_text()\n",
    "        except:\n",
    "            bookmarks=0\n",
    "        try:\n",
    "            hits=soup.find('dd', attrs={'class':'hits'}).get_text()\n",
    "        except:\n",
    "            hits=0\n",
    "        row = {'link':x, \n",
    "               'language':language,\n",
    "               'words':words,\n",
    "               'kudos':kudos, \n",
    "               'comments':comments,\n",
    "               'bookmarks':bookmarks,\n",
    "               'hits':hits\n",
    "               }\n",
    "        print(row)\n",
    "        data.append(row)\n",
    "        time.sleep(10)\n",
    "    return pd.DataFrame(data)\n",
    "data_list3=get_missing(data_list1)\n",
    "data_listfinal=data_list2.append(data_list3)\n",
    "data_listfinal[data_listfinal.isna().any(axis=1)]\n",
    "data_listfinal.to_csv('lockwood_part2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimal_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
