{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run selenium and chrome driver to scrape data from cloudbytes.dev\n",
    "import time\n",
    "import os.path\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.headless = True # Ensure GUI is off\n",
    "# chrome_options.add_argument(\"--window-size=1920,1200\")\n",
    "# Set path to chromedriver as per your configuration\n",
    "homedir = os.path.expanduser(\"~\")\n",
    "webdriver_service = Service(f\"{homedir}/ao3lockwood-co/chromedriver\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose Chrome Browser\n",
    "browser = webdriver.Chrome(service=webdriver_service, options=chrome_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(browser):\n",
    "    works = browser.find_elements(By.XPATH, '//ol[2]/li')\n",
    "    # Iterate through each work and extract author and datetime\n",
    "    data=[]\n",
    "    for work in works:\n",
    "        h4 = work.find_element(By.TAG_NAME,'h4')\n",
    "        a = h4.find_elements(By.TAG_NAME, 'a')\n",
    "        links = a[0].get_attribute(\"href\")\n",
    "        data.append(links)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_list_inclusion(list1, list2):\n",
    "    return all(item in list2 for item in list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tv_book(str,new_links):\n",
    "    for p in range(1, 20):\n",
    "        print(f'Processing page {p} of {str}')\n",
    "        if str=='tv':\n",
    "            link = f'https://archiveofourown.org/tags/Lockwood%20*a*%20Co*d*%20(TV)/works?commit=Sort+and+Filter&exclude_work_search[fandom_ids][]=1250871&page={p}&work_search[complete]=&work_search[crossover]=&work_search[date_from]=&work_search[date_to]=&work_search[excluded_tag_names]=&work_search[language_id]=&work_search[other_tag_names]=&work_search[query]=&work_search[sort_column]=created_at&work_search[words_from]=&work_search[words_to]='\n",
    "        elif str=='book':\n",
    "            link = f'https://archiveofourown.org/tags/Lockwood%20*a*%20Co*d*%20-%20Jonathan%20Stroud/works?commit=Sort+and+Filter&page={p}&work_search%5Bcomplete%5D=&work_search%5Bcrossover%5D=&work_search%5Bdate_from%5D=&work_search%5Bdate_to%5D=&work_search%5Bexcluded_tag_names%5D=&work_search%5Blanguage_id%5D=&work_search%5Bother_tag_names%5D=&work_search%5Bquery%5D=&work_search%5Bsort_column%5D=created_at&work_search%5Bwords_from%5D=&work_search%5Bwords_to%5D='\n",
    "        time.sleep(10)\n",
    "        browser.get(link)\n",
    "        temp_links = get_links(browser)\n",
    "        if check_list_inclusion(temp_links, links):\n",
    "            break\n",
    "        else:\n",
    "            new_links += temp_links\n",
    "    return new_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pages(links):\n",
    "    new_links = []\n",
    "    # for TV\n",
    "    new_links = process_tv_book('tv', new_links)\n",
    "    # for books\n",
    "    new_links = process_tv_book('book', new_links)\n",
    "    # remove duplicates\n",
    "    new_links = list(set(new_links))\n",
    "    data = pd.DataFrame(columns=['link','title','author','published','updatedate','chapters','language','words','kudos','comments','bookmarks','hits','warning','relationship','characters','tags','summary','rating','series'])\n",
    "    data['link'] = new_links\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data):\n",
    "    counter=0\n",
    "    slow_links = [] # List to store links that are taking too long to access\n",
    "    for x in range(len(data['link'])):\n",
    "        start_time = time.time()\n",
    "        if pd.isnull(data.loc[x,'summary']):\n",
    "            print(f\"getting missing data {x+1}/{len(data['link'])}\")\n",
    "            try:\n",
    "                newlink=data['link'][x]+'?view_adult=true'\n",
    "                page_start_time=time.time()\n",
    "                source = requests.get(newlink, headers={\n",
    "                              'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64)'}).text\n",
    "                elapsed_time = time.time() - page_start_time\n",
    "                if elapsed_time > 10:\n",
    "                    print(f\"Link {data['link'][x]} is taking too long to access. Adding to slow_links list.\")\n",
    "                    slow_links.append(data['link'][x])\n",
    "                    continue\n",
    "            except requests.exceptions.RequestException:\n",
    "                print(f\"Link {data['link'][x]} is taking too long to access. Adding to slow_links list.\")\n",
    "                slow_links.append(data['link'][x])\n",
    "                continue\n",
    "            soup = BeautifulSoup(source,'html.parser')\n",
    "            try:\n",
    "                data.loc[x,'title']=soup.find('h2', attrs={'class':'title heading'}).get_text().replace('\\n','').strip()\n",
    "            except:\n",
    "                data.loc[x,'title']=np.nan\n",
    "            try:\n",
    "                data.loc[x,'author']=soup.find('a', attrs={'rel':'author'}).get_text()\n",
    "            except:\n",
    "                data.loc[x,'author']=\"Anonymous\"\n",
    "            try:\n",
    "                data.loc[x,'published']=soup.find('dd', attrs={'class':'published'}).get_text()\n",
    "            except:\n",
    "                data.loc[x,'published']=np.nan\n",
    "            try:\n",
    "                data.loc[x,'updatedate'] = soup.find('dd', attrs={'class':'status'}).get_text()\n",
    "            except:\n",
    "                data.loc[x,'updatedate']=data['published'][x]\n",
    "            \n",
    "            try:\n",
    "                data.loc[x,'chapters']=soup.find('dd', attrs={'class':'chapters'}).get_text()\n",
    "            except:\n",
    "                data.loc[x,'chapters']=np.nan\n",
    "            \n",
    "            try:\n",
    "                data.loc[x,'language']=soup.find('dd', attrs={'class':'language'}).get_text().replace('\\n','').strip()\n",
    "            except:\n",
    "                data.loc[x,'language']=np.nan\n",
    "            \n",
    "            try:\n",
    "                data.loc[x,'words']=soup.find('dd', attrs={'class':'words'}).get_text()\n",
    "            except:\n",
    "                data.loc[x,'words']=np.nan\n",
    "            try:\n",
    "                data.loc[x,'kudos']=soup.find('dd', attrs={'class':'kudos'}).get_text()\n",
    "            except:\n",
    "                data.loc[x,'kudos']=0\n",
    "            try:\n",
    "                data.loc[x,'comments']=soup.find('dd', attrs={'class':'comments'}).get_text()\n",
    "            except:\n",
    "                data.loc[x,'comments']=0\n",
    "            try:\n",
    "                data.loc[x,'bookmarks']=soup.find('dd', attrs={'class':'bookmarks'}).get_text()\n",
    "            except:\n",
    "                data.loc[x,'bookmarks']=0\n",
    "            try:\n",
    "                data.loc[x,'hits']=soup.find('dd', attrs={'class':'hits'}).get_text()\n",
    "            except:\n",
    "                data.loc[x,'hits']=0\n",
    "            \n",
    "            try:\n",
    "                data.loc[x,'warning']=soup.find('dd', attrs={'class':'warning tags'}).get_text().replace('\\n','').strip()\n",
    "            except:\n",
    "                data.loc[x,'warning']=0\n",
    "            try:\n",
    "                r = soup.find('dd', attrs={'class':'relationship tags'})\n",
    "                relationships = r.find_all('li')\n",
    "                rel_list = []\n",
    "                for rel in relationships:\n",
    "                    rel_list.append(rel.get_text().strip())\n",
    "                data.loc[x,'relationship'] = ', '.join(rel_list)\n",
    "            except:\n",
    "                data.loc[x,'relationship'] = ''\n",
    "            try:\n",
    "                c = soup.find('dd', attrs={'class':'character tags'})\n",
    "                characters = c.find_all('li')\n",
    "                char_list = []\n",
    "                for char in characters:\n",
    "                    char_list.append(char.get_text().strip())\n",
    "                data.loc[x,'characters'] = ', '.join(char_list)\n",
    "            except:\n",
    "                data.loc[x,'characters']=''\n",
    "            try:\n",
    "                t = soup.find('dd', attrs={'class':'freeform tags'})\n",
    "                tags = t.find_all('li')\n",
    "                tag_list = []\n",
    "                for tag in tags:\n",
    "                    tag_list.append(tag.get_text().strip())\n",
    "                data.loc[x,'tags'] = ', '.join(tag_list)\n",
    "            except:\n",
    "                data.loc[x,'tags'] = ''\n",
    "            try:\n",
    "                data.loc[x,'series'] = soup.find('span', attrs={'class':'position'}).get_text().replace('\\n','').strip()\n",
    "            except:\n",
    "                data.loc[x,'series'] = 'not a series'\n",
    "            try:\n",
    "                data.loc[x,'summary']=soup.find('div', attrs={'class':'summary module'}).get_text().replace('\\n', ' ').replace('Summary:','').strip()\n",
    "            except:\n",
    "                data.loc[x,'summary']=np.nan\n",
    "            \n",
    "            try:\n",
    "                data.loc[x,'rating']=soup.find('dd', attrs={'class':'rating tags'}).get_text().replace('\\n','').strip()\n",
    "            except:\n",
    "                data.loc[x,'rating']=np.nan\n",
    "            print(data.iloc[x])\n",
    "            time.sleep(10)\n",
    "        elapsed_total_time = time.time() - start_time\n",
    "        if elapsed_total_time > 120*60:\n",
    "            for l in slow_links:\n",
    "                print(l)\n",
    "            return data\n",
    "    for l in slow_links:\n",
    "        print(l)\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_data(data):\n",
    "    counter=0\n",
    "    slow_links = [] # List to store links that are taking too long to access\n",
    "    for x in range(len(data['link'])):\n",
    "        start_time = time.time()\n",
    "        print(f\"updating data {x+1}/{len(data['link'])}\")\n",
    "        try:\n",
    "            newlink=data['link'][x]+'?view_adult=true'\n",
    "            page_start_time=time.time()\n",
    "            source = requests.get(newlink, headers={\n",
    "                        'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64)'}).text\n",
    "            elapsed_time = time.time() - page_start_time\n",
    "            if elapsed_time > 10:\n",
    "                print(f\"Link {data['link'][x]} is taking too long to access. Adding to slow_links list.\")\n",
    "                slow_links.append(data['link'][x])\n",
    "                continue\n",
    "        except requests.exceptions.RequestException:\n",
    "            print(f\"Link {data['link'][x]} is taking too long to access. Adding to slow_links list.\")\n",
    "            slow_links.append(data['link'][x])\n",
    "            continue\n",
    "        soup = BeautifulSoup(source,'html.parser')\n",
    "        try:\n",
    "            data.loc[x,'updatedate'] = soup.find('dd', attrs={'class':'status'}).get_text()\n",
    "        except:\n",
    "            data.loc[x,'updatedate']=data['published'][x]\n",
    "        try:\n",
    "            data.loc[x,'chapters']=soup.find('dd', attrs={'class':'chapters'}).get_text()\n",
    "        except:\n",
    "            data.loc[x,'chapters']=np.nan\n",
    "        try:\n",
    "            data.loc[x,'words']=soup.find('dd', attrs={'class':'words'}).get_text()\n",
    "        except:\n",
    "            data.loc[x,'words']=np.nan\n",
    "        try:\n",
    "            data.loc[x,'kudos']=soup.find('dd', attrs={'class':'kudos'}).get_text()\n",
    "        except:\n",
    "            data.loc[x,'kudos']=0\n",
    "        try:\n",
    "           data.loc[x,'comments']=soup.find('dd', attrs={'class':'comments'}).get_text()\n",
    "        except:\n",
    "              data.loc[x,'comments']=0\n",
    "        try:\n",
    "            data.loc[x,'bookmarks']=soup.find('dd', attrs={'class':'bookmarks'}).get_text()\n",
    "        except:\n",
    "            data.loc[x,'bookmarks']=0\n",
    "        try:\n",
    "            data.loc[x,'hits']=soup.find('dd', attrs={'class':'hits'}).get_text()\n",
    "        except:\n",
    "            data.loc[x,'hits']=0\n",
    "        try:\n",
    "            data.loc[x,'warning']=soup.find('dd', attrs={'class':'warning tags'}).get_text().replace('\\n','').strip()\n",
    "        except:\n",
    "            data.loc[x,'warning']=0\n",
    "        try:\n",
    "            t = soup.find('dd', attrs={'class':'freeform tags'})\n",
    "            tags = t.find_all('li')\n",
    "            tag_list = []\n",
    "            for tag in tags:\n",
    "                tag_list.append(tag.get_text().strip())\n",
    "                data.loc[x,'tags'] = ', '.join(tag_list)\n",
    "        except:\n",
    "            data.loc[x,'tags'] = ''\n",
    "        try:\n",
    "            data.loc[x,'rating']=soup.find('dd', attrs={'class':'rating tags'}).get_text().replace('\\n','').strip()\n",
    "        except:\n",
    "            data.loc[x,'rating']=np.nan\n",
    "            print(data.iloc[x])\n",
    "            time.sleep(10)\n",
    "        elapsed_total_time = time.time() - start_time\n",
    "    if elapsed_total_time > 120*60:\n",
    "        for l in slow_links:\n",
    "            print(l)\n",
    "        return data\n",
    "    for l in slow_links:\n",
    "        print(l)\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_string = '20230525_2238'\n",
    "filename=f'ao3_lockwood_and_co_ao_{dt_string}.csv'\n",
    "prev_df = pd.read_csv(f'AO3/{filename}')\n",
    "working_df = prev_df\n",
    "# datetime object containing current date and time\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%Y%m%d_%H%M\")\n",
    "print(\"date and time =\", dt_string)\n",
    "working_df = working_df[['link','title','author','published','updatedate','chapters','language','words','kudos','comments','bookmarks','hits','warning','relationship','characters','tags','summary','rating','series']]\n",
    "working_df = update_data(working_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = process_pages(working_df['link'])\n",
    "new_df = get_data(new_df)\n",
    "working_df = pd.concat([new_df, working_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=f'ao3_lockwood_and_co_ao_{dt_string}.csv'\n",
    "working_df = working_df.to_csv(f'AOE/{filename}', index=False)\n",
    "working_df = pd.read_csv(f'AO3/{filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the chapter column into chapter and chapter_max, and create a completion column\n",
    "working_df[['chapter','chapter_max']] = working_df.chapters.str.split(\"/\", expand=True)\n",
    "working_df['completion'] = working_df.apply(lambda row: 'completed' if row['chapter']==row['chapter_max'] else 'incomplete', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df['published'] = pd.to_datetime(working_df['published'])\n",
    "working_df['updatedate'] = pd.to_datetime(working_df['updatedate'])\n",
    "working_df['currentdate'] = max(working_df['updatedate'])\n",
    "working_df['datediff_pub'] = (working_df['currentdate']-working_df['published'])/np.timedelta64(1,'D')\n",
    "working_df['datediff'] = (working_df['currentdate']-working_df['updatedate'])/np.timedelta64(1,'D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df['classification'] = working_df.apply(lambda row: 'oneshot' if row['chapter_max']=='1' else ('multichapter(complete)' if row['completion']=='completed' else ('multichapter(updating)' if row['datediff']<=60 else 'multichapter(dormant)')), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_item(column):\n",
    "    item=[]\n",
    "    for row in column:\n",
    "        try:\n",
    "            row_item = row.replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\").replace('\"','').split(\",\")\n",
    "        except:\n",
    "            row_item = ['']\n",
    "        if row_item!=['']:\n",
    "            item.append(len(row_item))\n",
    "        else:\n",
    "            item.append(0) \n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_df = working_df.groupby(['author'], as_index=False).agg({'updatedate':'max', 'published':'min'})\n",
    "author_df = author_df.rename(columns={'updatedate':'lastauthorupdate','published':'firstauthorupdate'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'firstauthorupdate_x' in working_df.columns:\n",
    "    working_df=working_df.drop(columns=['firstauthorupdate_x','lastauthorupdate_x', 'lastauthorupdate_y','firstauthorupdate_y'])\n",
    "    working_df=working_df.merge(author_df, how='left', on='author')\n",
    "else:\n",
    "    working_df=working_df.merge(author_df, how='left', on='author')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df['author_lastupdate_diff'] = (working_df['currentdate']-working_df['lastauthorupdate'])/np.timedelta64(1,'D')\n",
    "working_df['daysactive'] = (working_df['lastauthorupdate']-working_df['firstauthorupdate'])/np.timedelta64(1,'D')\n",
    "working_df['daysincefirtupload'] = (working_df['currentdate']-working_df['firstauthorupdate'])/np.timedelta64(1,'D')\n",
    "working_df['author_activity'] = working_df['author_lastupdate_diff'].apply(lambda x: 'active' if x<=60 else 'inactive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df['num_relationship']=get_num_item(working_df['relationship'])\n",
    "working_df['num_characters']=get_num_item(working_df['characters'])\n",
    "working_df['num_tags']=get_num_item(working_df['tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_df = prev_df[['link', 'words']]\n",
    "prev_df = prev_df.rename(columns={'words':'prev_words'})\n",
    "working_df = working_df.merge(prev_df, how='left', on='link')\n",
    "working_df['prev_words'] = working_df['prev_words'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_item(id_column,item_column, name_col):\n",
    "    item_list=[]\n",
    "    for x in range(len(id_column)):\n",
    "        try:\n",
    "            row_item = item_column[x].replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\").replace('\"','').split(\",\")\n",
    "        except:\n",
    "            row_item = ['']\n",
    "        for item in row_item:\n",
    "            item=item.strip()\n",
    "            if '&' not in item:\n",
    "                item_list.append([id_column[x],item])\n",
    "    return pd.DataFrame(item_list, columns = ['link', name_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_df = get_df_item(working_df['link'], working_df['characters'], 'charactername')\n",
    "character = pd.read_csv('AO3/characters.csv')\n",
    "char_df =char_df.merge(character, how='left', on='charactername')\n",
    "char_df['character'] = char_df['character'].fillna(char_df['charactername'])\n",
    "char_df=char_df.drop(columns='charactername')\n",
    "char_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationship_df = get_df_item(working_df['link'], working_df['relationship'], 'shiptag')\n",
    "relationship = pd.read_csv('AO3/relationships.csv')\n",
    "relationship_df =relationship_df.merge(relationship, how='left', on='shiptag')\n",
    "relationship_df['ship'] = relationship_df['ship'].fillna(relationship_df['shiptag'])\n",
    "relationship_df=relationship_df.drop(columns='shiptag')\n",
    "relationship_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_df= get_df_item(working_df['link'], working_df['tags'], 'tag_item')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_rel_tag = char_df.merge(relationship_df, how='outer', on='link')\n",
    "char_rel_tag = char_rel_tag.merge(tags_df, how='outer', on='link')\n",
    "char_rel_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_rel_tag.to_csv('AO3/character_relationship_tags.csv', index=False)\n",
    "working_df = pd.read_csv(f'AO3/{filename}', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
