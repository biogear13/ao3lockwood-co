{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run selenium and chrome driver to scrape data from cloudbytes.dev\n",
    "import time\n",
    "import os.path\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import sqlite3\n",
    "import chromedriver_autoinstaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.headless = True # Ensure GUI is off\n",
    "chrome_options.add_argument(\"--window-size=1920,1200\")\n",
    "# Set path to chromedriver as per your configuration\n",
    "#homedir = os.path.expanduser(\"~\")\n",
    "#webdriver_service = Service(f\"{homedir}/ao3lockwood-co/chromedriver\")\n",
    "chromedriver_autoinstaller.install()\n",
    "\n",
    "# Initialize Chrome browser\n",
    "browser = webdriver.Chrome(options=chrome_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            link\n",
      "0     https://archiveofourown.org/works/44791411\n",
      "1     https://archiveofourown.org/works/47484628\n",
      "2     https://archiveofourown.org/works/45235999\n",
      "3     https://archiveofourown.org/works/45056584\n",
      "4     https://archiveofourown.org/works/47517634\n",
      "...                                          ...\n",
      "2026  https://archiveofourown.org/works/47837212\n",
      "2027  https://archiveofourown.org/works/47802628\n",
      "2028  https://archiveofourown.org/works/45773677\n",
      "2029  https://archiveofourown.org/works/46481659\n",
      "2030  https://archiveofourown.org/works/47821801\n",
      "\n",
      "[2031 rows x 1 columns]\n",
      "    author_id              author joined_ao3\n",
      "0        None           OutofOz23       None\n",
      "1        None        andriewskaja       None\n",
      "2        None     Stars_that_fall       None\n",
      "3        None       blackwoodhart       None\n",
      "4        None         OceanSpray5       None\n",
      "..        ...                 ...        ...\n",
      "663      None      fictophilegirl       None\n",
      "664      None          Pickl3lily       None\n",
      "665      None              czenzo       None\n",
      "666      None  therealjanebingley       None\n",
      "667      None        IrisPurpurea       None\n",
      "\n",
      "[668 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# get the links from the database\n",
    "conn = sqlite3.connect('output/ao3.db')\n",
    "querry = \"SELECT link FROM fanfic\"\n",
    "link_list = pd.read_sql_query(querry, conn)\n",
    "print(link_list)\n",
    "# get the list of authors from the database\n",
    "querry = \"SELECT * FROM authors\"\n",
    "author_list = pd.read_sql_query(querry, conn)\n",
    "print(author_list)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(browser):\n",
    "    # Find all the works using XPath\n",
    "    works = browser.find_elements(By.XPATH, '//ol[2]/li')\n",
    "\n",
    "    # Iterate through each work and extract author and datetime\n",
    "    data = []\n",
    "    for work in works:\n",
    "        h4 = work.find_element(By.TAG_NAME, 'h4')\n",
    "        a = h4.find_elements(By.TAG_NAME, 'a')\n",
    "        # Get the href attribute of the first <a> tag\n",
    "        link = a[0].get_attribute(\"href\")\n",
    "        data.append(link)\n",
    "        #print(link)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tv_book(category, links):\n",
    "    temp_links = []\n",
    "    if category == \"books\":\n",
    "        for page in range(1, 11):\n",
    "            print(f'processing page {page}')\n",
    "            link = f'https://archiveofourown.org/tags/Lockwood%20*a*%20Co*d*%20-%20Jonathan%20Stroud/works?page={page}'\n",
    "            time.sleep(10)\n",
    "            browser.get(link)\n",
    "            temp_links+=(get_links(browser))\n",
    "    elif category == \"tv\":\n",
    "        for page in range(1, 4):\n",
    "            print(f'processing page {page}')\n",
    "            link = f'https://archiveofourown.org/tags/Lockwood%20*a*%20Co*d*%20(TV)/works?commit=Sort+and+Filter&exclude_work_search%5Bfandom_ids%5D%5B%5D=1250871&page={page}&work_search%5Bcomplete%5D=&work_search%5Bcrossover%5D=&work_search%5Bdate_from%5D=&work_search%5Bdate_to%5D=&work_search%5Bexcluded_tag_names%5D=&work_search%5Blanguage_id%5D=&work_search%5Bother_tag_names%5D=&work_search%5Bquery%5D=&work_search%5Bsort_column%5D=revised_at&work_search%5Bwords_from%5D=&work_search%5Bwords_to%5D='\n",
    "            time.sleep(10)\n",
    "            browser.get(link)\n",
    "            temp_links+=(get_links(browser))\n",
    "    update_links = list(set(temp_links) & set(links))\n",
    "    print(f'found {len(update_links)} updated links')\n",
    "    new_links = list(set(temp_links) - set(links))\n",
    "    print(f'found {len(new_links)} new links')\n",
    "    return update_links, new_links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing page 1\n",
      "https://archiveofourown.org/tags/Lockwood%20*a*%20Co*d*%20(TV)/works?commit=Sort+and+Filter&exclude_work_search%5Bfandom_ids%5D%5B%5D=1250871&page=1&work_search%5Bcomplete%5D=&work_search%5Bcrossover%5D=&work_search%5Bdate_from%5D=&work_search%5Bdate_to%5D=&work_search%5Bexcluded_tag_names%5D=&work_search%5Blanguage_id%5D=&work_search%5Bother_tag_names%5D=&work_search%5Bquery%5D=&work_search%5Bsort_column%5D=revised_at&work_search%5Bwords_from%5D=&work_search%5Bwords_to%5D=\n",
      "processing page 2\n",
      "https://archiveofourown.org/tags/Lockwood%20*a*%20Co*d*%20(TV)/works?commit=Sort+and+Filter&exclude_work_search%5Bfandom_ids%5D%5B%5D=1250871&page=2&work_search%5Bcomplete%5D=&work_search%5Bcrossover%5D=&work_search%5Bdate_from%5D=&work_search%5Bdate_to%5D=&work_search%5Bexcluded_tag_names%5D=&work_search%5Blanguage_id%5D=&work_search%5Bother_tag_names%5D=&work_search%5Bquery%5D=&work_search%5Bsort_column%5D=revised_at&work_search%5Bwords_from%5D=&work_search%5Bwords_to%5D=\n",
      "processing page 3\n",
      "https://archiveofourown.org/tags/Lockwood%20*a*%20Co*d*%20(TV)/works?commit=Sort+and+Filter&exclude_work_search%5Bfandom_ids%5D%5B%5D=1250871&page=3&work_search%5Bcomplete%5D=&work_search%5Bcrossover%5D=&work_search%5Bdate_from%5D=&work_search%5Bdate_to%5D=&work_search%5Bexcluded_tag_names%5D=&work_search%5Blanguage_id%5D=&work_search%5Bother_tag_names%5D=&work_search%5Bquery%5D=&work_search%5Bsort_column%5D=revised_at&work_search%5Bwords_from%5D=&work_search%5Bwords_to%5D=\n",
      "found 47 updated links\n",
      "found 13 new links\n"
     ]
    }
   ],
   "source": [
    "tv_new_links, tv_update_links = process_tv_book(\"tv\", link_list.link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing page 1\n",
      "processing page 2\n",
      "processing page 3\n",
      "processing page 4\n",
      "processing page 5\n",
      "processing page 6\n",
      "processing page 7\n",
      "processing page 8\n",
      "processing page 9\n",
      "processing page 10\n",
      "found 106 updated links\n",
      "found 94 new links\n"
     ]
    }
   ],
   "source": [
    "book_new_links, book_update_links = process_tv_book(\"books\", link_list.link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seriestags(links, series, collections, pairings, characters, relationships, tags):\n",
    "    for x in range(len(links)):\n",
    "        print(f\"getting fanfic {x+1}/{len(links)}\")\n",
    "        try:\n",
    "            newlink=links.loc[x,'link']+'?view_adult=true'\n",
    "            fanfic = requests.get(newlink, headers={\n",
    "                          'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64)'}).text\n",
    "        except requests.exceptions.RequestException:\n",
    "            print(f\"fanfic {links.loc[x,'link']} is taking too long to access.\")\n",
    "            continue\n",
    "        fanfic = BeautifulSoup(fanfic,'html.parser')\n",
    "        try:\n",
    "            series_names = fanfic.find('dd', attrs={'class':'series'})\n",
    "            series_names = series_names.find_all('span', attrs={'class':'position'})\n",
    "            new_rows = []\n",
    "            for series_name in series_names:\n",
    "                new_row = [links.loc[x,'fanfic_id'], series_name.find('a').get_text()]\n",
    "                new_rows.append(new_row)\n",
    "            series = series.append(pd.DataFrame(new_rows, columns=series.columns))\n",
    "            series = series.drop_duplicates()\n",
    "        except:\n",
    "            print('not in a series')\n",
    "        try:\n",
    "            collection_names = fanfic.find('dd', attrs={'class':'collections'})\n",
    "            collection_names = collection_names.find_all('a')\n",
    "            new_rows = []\n",
    "            for collection_name in collection_names:\n",
    "                new_row = [links.loc[x,'fanfic_id'], collection_name.get_text()]\n",
    "                new_rows.append(new_row)\n",
    "            collections = collections.append(pd.DataFrame(new_rows, columns=collections.columns))\n",
    "            collections = collections.drop_duplicates()\n",
    "        except:\n",
    "            print('not in a collection')\n",
    "        try:\n",
    "            pairing_names = fanfic.find('dd', attrs={'class':'category tags'})\n",
    "            pairing_names = pairing_names.find_all('a', attrs={'class':'tag'})\n",
    "            new_rows = []\n",
    "            for pairing_name in pairing_names:\n",
    "                new_row = [links.loc[x,'fanfic_id'], pairing_name.get_text()]\n",
    "                new_rows.append(new_row)\n",
    "            pairings = pairings.append(pd.DataFrame(new_rows, columns=characters.columns))\n",
    "            pairings = pairings.drop_duplicates()\n",
    "        except:\n",
    "            print('no pairing tags')\n",
    "        try:\n",
    "            update_date = fanfic.find('dd', attrs={'class':'status'}).get_text()\n",
    "        except:\n",
    "            try:\n",
    "                update_date = fanfic.find('dd', attrs={'class':'published'}).get_text()\n",
    "            except:\n",
    "                update_date = np.nan\n",
    "                print('no update date')\n",
    "        try:\n",
    "            character_names = fanfic.find('dd', attrs={'class':'character tags'})\n",
    "            character_names = character_names.find_all('a', attrs={'class':'tag'})\n",
    "            new_rows = []\n",
    "            for character_name in character_names:\n",
    "                new_row = [links.loc[x,'fanfic_id'], update_date, character_name.get_text()]\n",
    "                new_rows.append(new_row)\n",
    "            characters = characters.append(pd.DataFrame(new_rows, columns=characters.columns))\n",
    "            characters = characters.drop_duplicates()\n",
    "        except:\n",
    "            print('no character tags')\n",
    "        try:\n",
    "            relationship_names = fanfic.find('dd', attrs={'class':'relationship tags'})\n",
    "            relationship_names = relationship_names.find_all('a', attrs={'class':'tag'})\n",
    "            new_rows = []\n",
    "            for relationship_name in relationship_names:\n",
    "                new_row = [links.loc[x,'fanfic_id'], update_date, relationship_name.get_text()]\n",
    "                new_rows.append(new_row)\n",
    "            relationships = relationships.append(pd.DataFrame(new_rows, columns=relationships.columns))\n",
    "            relationships = relationships.drop_duplicates()\n",
    "        except:\n",
    "            print('no relationship tags')\n",
    "        try:\n",
    "            tag_names = fanfic.find('dd', attrs={'class':'freeform tags'})\n",
    "            tag_names = tag_names.find_all('a', attrs={'class':'tag'})\n",
    "            new_rows = []\n",
    "            for tag_name in tag_names:\n",
    "                new_row = [links.loc[x,'fanfic_id'], update_date, tag_name.get_text()]\n",
    "                new_rows.append(new_row)\n",
    "            tags = tags.append(pd.DataFrame(new_rows, columns=tags.columns))\n",
    "            tags = tags.drop_duplicates()\n",
    "        except:\n",
    "            print('no tags')\n",
    "        time.sleep(10)\n",
    "    return series, collections, pairings, characters, relationships, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_series(fanfic, fanfic_id, series):\n",
    "    try:\n",
    "        series_names = fanfic.find('dd', attrs={'class':'series'})\n",
    "        series_names = series_names.find_all('span', attrs={'class':'position'})\n",
    "        new_rows = []\n",
    "        for series_name in series_names:\n",
    "            new_row = [fanfic_id, series_name.find('a').get_text()]\n",
    "            new_rows.append(new_row)\n",
    "        series = series.append(pd.DataFrame(new_rows, columns=series.columns))\n",
    "        series = series.drop_duplicates()\n",
    "    except:\n",
    "        print('not in a series')\n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collections(fanfic, fanfic_id, collections):\n",
    "    try:\n",
    "        collection_names = fanfic.find('dd', attrs={'class':'collections'})\n",
    "        collection_names = collection_names.find_all('a')\n",
    "        new_rows = []\n",
    "        for collection_name in collection_names:\n",
    "            new_row = [fanfic_id, collection_name.get_text()]\n",
    "            new_rows.append(new_row)\n",
    "        collections = collections.append(pd.DataFrame(new_rows, columns=collections.columns))\n",
    "        collections = collections.drop_duplicates()\n",
    "    except:\n",
    "        print('not in a collection')\n",
    "    return collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairings(fanfic, fanfic_id, pairings):\n",
    "    try:\n",
    "        pairing_names = fanfic.find('dd', attrs={'class':'category tags'})\n",
    "        pairing_names = pairing_names.find_all('a', attrs={'class':'tag'})\n",
    "        new_rows = []\n",
    "        for pairing_name in pairing_names:\n",
    "            new_row = [fanfic_id, pairing_name.get_text()]\n",
    "            new_rows.append(new_row)\n",
    "        pairings = pairings.append(pd.DataFrame(new_rows, columns=pairings.columns))\n",
    "        pairings = pairings.drop_duplicates()\n",
    "    except:\n",
    "        print('no pairing tags')\n",
    "    return pairings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_update_date(fanfic):\n",
    "    try:\n",
    "        update_date = fanfic.find('dd', attrs={'class':'status'}).get_text()\n",
    "    except:\n",
    "        try:\n",
    "            update_date = fanfic.find('dd', attrs={'class':'published'}).get_text()\n",
    "        except:\n",
    "            update_date = np.nan\n",
    "            print('no update date')\n",
    "    return update_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_characters(fanfic, fanfic_id, characters, update_date):\n",
    "    try:\n",
    "        character_names = fanfic.find('dd', attrs={'class':'character tags'})\n",
    "        character_names = character_names.find_all('a', attrs={'class':'tag'})\n",
    "        new_rows = []\n",
    "        for character_name in character_names:\n",
    "            new_row = [fanfic_id, update_date, character_name.get_text()]\n",
    "            new_rows.append(new_row)\n",
    "        characters = characters.append(pd.DataFrame(new_rows, columns=characters.columns))\n",
    "        characters = characters.drop_duplicates()\n",
    "    except:\n",
    "        print('no character tags')\n",
    "    return characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relationships(fanfic, fanfic_id, update_date, relationships):\n",
    "    try:\n",
    "        relationship_names = fanfic.find('dd', attrs={'class':'relationship tags'})\n",
    "        relationship_names = relationship_names.find_all('a', attrs={'class':'tag'})\n",
    "        new_rows = []\n",
    "        for relationship_name in relationship_names:\n",
    "            new_row = [fanfic_id, update_date, relationship_name.get_text()]\n",
    "            new_rows.append(new_row)\n",
    "        relationships = relationships.append(pd.DataFrame(new_rows, columns=relationships.columns))\n",
    "        relationships = relationships.drop_duplicates()\n",
    "    except:\n",
    "        print('no relationship tags')\n",
    "    return relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tags(fanfic, fanfic_id, update_date, tags):\n",
    "    try:\n",
    "        tag_names = fanfic.find('dd', attrs={'class':'freeform tags'})\n",
    "        tag_names = tag_names.find_all('a', attrs={'class':'tag'})\n",
    "        new_rows = []\n",
    "        for tag_name in tag_names:\n",
    "            new_row = [fanfic_id, update_date, tag_name.get_text()]\n",
    "            new_rows.append(new_row)\n",
    "        tags = tags.append(pd.DataFrame(new_rows, columns=tags.columns))\n",
    "        tags = tags.drop_duplicates()\n",
    "    except:\n",
    "        print('no tags')\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_authors(authors, fandoms):\n",
    "    for x in range(len(authors)):\n",
    "        print(f\"getting author {x+1}/{len(authors)}\")\n",
    "        try:\n",
    "            newlink=f\"https://archiveofourown.org/users/{authors.loc[x,'author']}/profile\"\n",
    "            profile = requests.get(newlink, headers={\n",
    "                          'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64)'}).text\n",
    "        except requests.exceptions.RequestException:\n",
    "            print(f\"author {authors['author'][x]} fandom is taking too long to access.\")\n",
    "            continue\n",
    "        profile = BeautifulSoup(profile,'html.parser')\n",
    "        #print(newlink)\n",
    "        try:\n",
    "            pseuds = profile.find('dl', attrs={'class':'meta'})\n",
    "            \n",
    "            pseuds = pseuds.find_all('dd')\n",
    "            joined = pseuds[1].get_text()\n",
    "            id = pseuds[2].get_text()\n",
    "        except:\n",
    "            joined = np.nan\n",
    "            id = np.nan\n",
    "        print(id, joined)\n",
    "        authors.loc[x,'author_id'] = id\n",
    "        authors.loc[x,'joined_ao3'] = joined\n",
    "        time.sleep(10)\n",
    "        try:\n",
    "            newlink=f\"https://archiveofourown.org/users/{authors.loc[x,'author']}\"\n",
    "            user = requests.get(newlink, headers={\n",
    "                          'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64)'}).text\n",
    "        except requests.exceptions.RequestException:\n",
    "            print(f\"author {authors['author'][x]} profile is taking too long to access.\")\n",
    "            continue\n",
    "        user = BeautifulSoup(user,'html.parser')\n",
    "        #print(newlink)\n",
    "        try:\n",
    "            index = user.find('ol', attrs={'class':'index group'})\n",
    "            index = index.find_all('li')\n",
    "            for i in range(len(index)):\n",
    "                li = index[i].get_text().split('(')\n",
    "                fandom = li[0].strip()\n",
    "                num_fic = li[-1].split(')')[0]\n",
    "                fandoms.loc[len(fandoms)] = [id, fandom, num_fic]\n",
    "        except:\n",
    "            print(f\"author {authors['author'][x]} fandom is taking too long to access.\")\n",
    "        time.sleep(10)\n",
    "    return authors, fandoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fanfic_data(story, fanfic, fanfic_id,  link):\n",
    "    try:\n",
    "        title = story.find('h2', attrs={'class':'title heading'}).get_text().replace('\\n','').strip()\n",
    "    except:\n",
    "        title = np.nan\n",
    "    try:\n",
    "        author = story.find('a', attrs={'rel':'author'}).get_text()\n",
    "    except:\n",
    "        author = np.nan\n",
    "    try:\n",
    "        published = story.find('dd', attrs={'class':'published'}).get_text()\n",
    "    except:\n",
    "        published = np.nan\n",
    "    try:\n",
    "        language = story.find('dd', attrs={'class':'language'}).get_text().replace('\\n','').strip()\n",
    "    except:\n",
    "        language = np.nan\n",
    "    try:\n",
    "        summary = story.find('div', attrs={'class':'summary module'}).get_text().replace('\\n', ' ').replace('Summary:','').strip()\n",
    "    except:\n",
    "        summary = np.nan\n",
    "    try:\n",
    "        warning = story.find('dd', attrs={'class':'warning tags'}).get_text().replace('\\n','').strip()\n",
    "    except:\n",
    "        warning = np.nan\n",
    "    new_row = [fanfic_id, link, title, author, published, language, summary, warning]\n",
    "    fanfic = fanfic.append(pd.DataFrame([new_row], columns=fanfic.columns))\n",
    "    return fanfic\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_updates(story, fanfic_id, update_date, updates):\n",
    "    try:\n",
    "        words=story.find('dd', attrs={'class': 'words'}).get_text()\n",
    "    except:\n",
    "        words=np.nan\n",
    "    try:\n",
    "        chapters=story.find('dd', attrs={'class': 'chapters'}).get_text()\n",
    "    except:\n",
    "        chapters=np.nan\n",
    "    chapter, chapter_max = chapters.split('/')\n",
    "    try:\n",
    "        rating= story.find('dd', attrs={'class':'rating tags'}).get_text().replace('\\n','').strip()  \n",
    "    except:\n",
    "        rating=np.nan\n",
    "    new_row = [fanfic_id, update_date, words, chapter, chapter_max, rating]\n",
    "    updates = updates.append(pd.DataFrame([new_row], columns=updates.columns))\n",
    "    return updates     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_engagement(story, fanfic_id, user_engagement):\n",
    "    try:\n",
    "        kudos=story.find('dd', attrs={'class': 'kudos'}).get_text()\n",
    "    except:\n",
    "        kudos=np.nan\n",
    "    try:\n",
    "        bookmarks=story.find('dd', attrs={'class': 'bookmarks'}).get_text()\n",
    "    except:\n",
    "        bookmarks=np.nan\n",
    "    try:\n",
    "        comments=story.find('dd', attrs={'class': 'comments'}).get_text()\n",
    "    except:\n",
    "        comments=np.nan\n",
    "    try:\n",
    "        hits=story.find('dd', attrs={'class': 'hits'}).get_text()\n",
    "    except:\n",
    "        hits=np.nan\n",
    "    \n",
    "    new_row = [fanfic_id, kudos, bookmarks, comments, hits]\n",
    "    user_engagement = user_engagement.append(pd.DataFrame([new_row], columns=user_engagement.columns))\n",
    "    return user_engagement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_data(new_links, author_list):\n",
    "    authors = pd.DataFrame(columns= ['author_id','author','joined_ao3'])\n",
    "    fandoms = pd.DataFrame(columns=['author_id', 'fandom', 'num_fic'])\n",
    "    fanfic = pd.DataFrame(columns=['fanfic_id','link','title','author','published','language','summary', 'warning'])\n",
    "    series = pd.DataFrame(columns=['fanfic_id', 'series_name'])\n",
    "    collections = pd.DataFrame(columns=['fanfic_id', 'collection_name'])\n",
    "    pairings = pd.DataFrame(columns=['fanfic_id', 'pairing_name'])\n",
    "    characters = pd.DataFrame(columns=['fanfic_id', 'update_date', 'character'])\n",
    "    relationships = pd.DataFrame(columns=['fanfic_id', 'update_date', 'relationship'])\n",
    "    tags = pd.DataFrame(columns=['fanfic_id', 'update_date', 'tag'])\n",
    "    updates = pd.DataFrame(columns=['fanfic_id','update_date','words', 'chapter', 'chapter_max','rating'])\n",
    "    for link in new_links:\n",
    "        try:\n",
    "            newlink=link+'?view_adult=true'\n",
    "            story = requests.get(newlink, headers={\n",
    "                          'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64)'}).text\n",
    "        except requests.exceptions.RequestException:\n",
    "            print(f\"fanfic {link} is taking too long to access.\")\n",
    "            continue\n",
    "        story = BeautifulSoup(story,'html.parser')\n",
    "        fanfic_id = link.replace('https://archiveofourown.org/works/','')\n",
    "        fanfic = get_fanfic_data(story, fanfic, fanfic_id, link)\n",
    "        series = get_series(story, fanfic_id, series)\n",
    "        collections = get_collections(story, fanfic_id, collections)\n",
    "        pairings = get_pairings(story, fanfic_id, pairings)\n",
    "        update_date = get_update_date(story)\n",
    "        characters = get_characters(story, fanfic_id, characters, update_date)\n",
    "        relationships = get_relationships(story, fanfic_id, update_date, relationships)\n",
    "        tags = get_tags(story, fanfic_id, update_date, tags)\n",
    "        updates = get_updates(story, fanfic_id, update_date, updates)\n",
    "        time.sleep(10)\n",
    "    author_missing = list(set(fanfic.author) - set(author_list.author))\n",
    "    authors['author'] = author_missing\n",
    "    authors, fandoms = get_authors(authors, fandoms)\n",
    "    authors['author_id'] = authors['author_id'].astype('str')\n",
    "    authors['authors'] = authors['authors'].astype('str')\n",
    "    fanfic['author'] = fanfic['author'].astype('str')\n",
    "    fanfic = fanfic.merge(authors[['author_id', 'author']], how='left', on='author')\n",
    "    fanfic = fanfic.drop(columns=['author'])\n",
    "    updates['update_date'] = pd.to_datetime(updates['update_date'])\n",
    "    updates['date_scraped'] = updates['update_date'].max()\n",
    "    conn = sqlite3.connect('output/ao3.db')\n",
    "    fanfic.to_sql('fanfic', conn, if_exists='append', index=False)\n",
    "    series.to_sql('series', conn, if_exists='append', index=False)\n",
    "    collections.to_sql('collections', conn, if_exists='append', index=False)\n",
    "    pairings.to_sql('pairings', conn, if_exists='append', index=False)\n",
    "    characters.to_sql('characters', conn, if_exists='append', index=False)\n",
    "    relationships.to_sql('relationships', conn, if_exists='append', index=False)\n",
    "    tags.to_sql('tags', conn, if_exists='append', index=False)\n",
    "    updates.to_sql('updates', conn, if_exists='append', index=False)\n",
    "    conn.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not in a series\n",
      "not in a collection\n",
      "not in a series\n",
      "not in a collection\n",
      "not in a series\n",
      "not in a collection\n",
      "not in a series\n",
      "not in a series\n",
      "not in a collection\n",
      "not in a series\n",
      "not in a collection\n",
      "not in a series\n",
      "not in a collection\n",
      "not in a series\n",
      "not in a collection\n",
      "no pairing tags\n",
      "no relationship tags\n",
      "not in a series\n",
      "not in a collection\n",
      "not in a collection\n",
      "not in a series\n",
      "not in a collection\n",
      "not in a series\n",
      "no relationship tags\n",
      "not in a series\n",
      "not in a series\n",
      "not in a collection\n",
      "no pairing tags\n",
      "not in a series\n",
      "not in a collection\n",
      "not in a series\n",
      "not in a series\n",
      "not in a collection\n",
      "not in a collection\n",
      "not in a collection\n",
      "not in a collection\n",
      "not in a series\n",
      "not in a collection\n",
      "not in a series\n",
      "not in a collection\n",
      "not in a series\n",
      "no relationship tags\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10301/4149347806.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_new_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtv_new_links\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthor_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mget_new_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbook_new_links\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthor_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_10301/2135013213.py\u001b[0m in \u001b[0;36mget_new_data\u001b[0;34m(new_links, author_list)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfanfic_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mupdates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_updates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfanfic_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mauthor_missing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfanfic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauthor_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mauthors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'author'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauthor_missing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "get_new_data(tv_new_links, author_list)\n",
    "get_new_data(book_new_links, author_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_up(update_links):\n",
    "    updates = pd.DataFrame(columns=['fanfic_id','update_date','words', 'chapter', 'chapter_max','rating'])\n",
    "    date_now = datetime.now()\n",
    "    prior_date = date_now - timedelta(days=8)\n",
    "    prior_date = prior_date.strftime(\"%m-%d-%Y\")\n",
    "    print(update_links)\n",
    "    for link in update_links:\n",
    "        try:\n",
    "            print(link)\n",
    "            newlink=link+'?view_adult=true'\n",
    "            story = requests.get(newlink, headers={\n",
    "                          'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64)'}).text\n",
    "        except requests.exceptions.RequestException:\n",
    "            print(f\"fanfic {link} is taking too long to access.\")\n",
    "            continue\n",
    "        story = BeautifulSoup(story,'html.parser')\n",
    "        fanfic_id = link.replace('https://archiveofourown.org/works/','')\n",
    "        update_date = get_update_date(story)\n",
    "        if update_date < prior_date:\n",
    "            updates = get_updates(story, fanfic_id, update_date, updates)\n",
    "        else:\n",
    "            break\n",
    "        time.sleep(10)\n",
    "    updates['update_date'] = pd.to_datetime(updates['update_date'])\n",
    "    updates['date_scraped'] = updates['update_date'].max()\n",
    "    print(updates)\n",
    "    conn = sqlite3.connect('output/ao3.db')\n",
    "    querry = \"SELECT * FROM updates\"\n",
    "    updates=updates.append(pd.read_sql_query(querry, conn), ignore_index=True)\n",
    "    updates = updates.drop_duplicates()\n",
    "    updates.to_sql('updates', conn, if_exists='append', index=False)\n",
    "    conn.close()\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://archiveofourown.org/works/48083995', 'https://archiveofourown.org/works/47908165', 'https://archiveofourown.org/works/47963101', 'https://archiveofourown.org/works/47850358', 'https://archiveofourown.org/works/48038953', 'https://archiveofourown.org/works/48059404', 'https://archiveofourown.org/works/47963587', 'https://archiveofourown.org/works/48019768', 'https://archiveofourown.org/works/47965579', 'https://archiveofourown.org/works/47990719', 'https://archiveofourown.org/works/48098992', 'https://archiveofourown.org/works/47989714', 'https://archiveofourown.org/works/48028060']\n",
      "https://archiveofourown.org/works/48083995\n",
      "Empty DataFrame\n",
      "Columns: [fanfic_id, update_date, words, chapter, chapter_max, rating, date_scraped]\n",
      "Index: []\n",
      "['https://archiveofourown.org/works/47917651', 'https://archiveofourown.org/works/48037162', 'https://archiveofourown.org/works/47974699', 'https://archiveofourown.org/works/47966419', 'https://archiveofourown.org/works/48025156', 'https://archiveofourown.org/works/48010030', 'https://archiveofourown.org/works/48040444', 'https://archiveofourown.org/works/48008644', 'https://archiveofourown.org/works/48004363', 'https://archiveofourown.org/works/48072769', 'https://archiveofourown.org/works/47950627', 'https://archiveofourown.org/works/48078949', 'https://archiveofourown.org/works/48016129', 'https://archiveofourown.org/works/48023284', 'https://archiveofourown.org/works/47996833', 'https://archiveofourown.org/works/47996479', 'https://archiveofourown.org/works/48096337', 'https://archiveofourown.org/works/48002539', 'https://archiveofourown.org/works/48082909', 'https://archiveofourown.org/works/47996065', 'https://archiveofourown.org/works/48023983', 'https://archiveofourown.org/works/47968216', 'https://archiveofourown.org/works/48023206', 'https://archiveofourown.org/works/48006607', 'https://archiveofourown.org/works/48052411', 'https://archiveofourown.org/works/48096685', 'https://archiveofourown.org/works/48095515', 'https://archiveofourown.org/works/48013183', 'https://archiveofourown.org/works/48000253', 'https://archiveofourown.org/works/48031762', 'https://archiveofourown.org/works/48000049', 'https://archiveofourown.org/works/48005476', 'https://archiveofourown.org/works/47928916', 'https://archiveofourown.org/works/47986495', 'https://archiveofourown.org/works/48076186', 'https://archiveofourown.org/works/48042094', 'https://archiveofourown.org/works/48053065', 'https://archiveofourown.org/works/47476237', 'https://archiveofourown.org/works/48048670', 'https://archiveofourown.org/works/47951596', 'https://archiveofourown.org/works/48010930', 'https://archiveofourown.org/works/48047875', 'https://archiveofourown.org/works/47985526', 'https://archiveofourown.org/works/47964064', 'https://archiveofourown.org/works/48064879', 'https://archiveofourown.org/works/48085939', 'https://archiveofourown.org/works/48053110', 'https://archiveofourown.org/works/47949136', 'https://archiveofourown.org/works/47961637', 'https://archiveofourown.org/works/47924617', 'https://archiveofourown.org/works/47940577', 'https://archiveofourown.org/works/48085003', 'https://archiveofourown.org/works/48083566', 'https://archiveofourown.org/works/47981308', 'https://archiveofourown.org/works/48088129', 'https://archiveofourown.org/works/47966824', 'https://archiveofourown.org/works/47975755', 'https://archiveofourown.org/works/48078289', 'https://archiveofourown.org/works/47923171', 'https://archiveofourown.org/works/48074035', 'https://archiveofourown.org/works/47850496', 'https://archiveofourown.org/works/48033853', 'https://archiveofourown.org/works/48002926', 'https://archiveofourown.org/works/47988043', 'https://archiveofourown.org/works/48035086', 'https://archiveofourown.org/works/48045670', 'https://archiveofourown.org/works/47980660', 'https://archiveofourown.org/works/48072469', 'https://archiveofourown.org/works/48015346', 'https://archiveofourown.org/works/48060256', 'https://archiveofourown.org/works/47929756', 'https://archiveofourown.org/works/47928415', 'https://archiveofourown.org/works/48068158', 'https://archiveofourown.org/works/48091924', 'https://archiveofourown.org/works/47977333', 'https://archiveofourown.org/works/48030172', 'https://archiveofourown.org/works/48055312', 'https://archiveofourown.org/works/48026956', 'https://archiveofourown.org/works/48037726', 'https://archiveofourown.org/works/48053056', 'https://archiveofourown.org/works/48019540', 'https://archiveofourown.org/works/48004510', 'https://archiveofourown.org/works/48068833', 'https://archiveofourown.org/works/48060433', 'https://archiveofourown.org/works/47906674', 'https://archiveofourown.org/works/48062422', 'https://archiveofourown.org/works/48023629', 'https://archiveofourown.org/works/47989165', 'https://archiveofourown.org/works/48009463', 'https://archiveofourown.org/works/48025828', 'https://archiveofourown.org/works/48076579', 'https://archiveofourown.org/works/48061528', 'https://archiveofourown.org/works/48054592', 'https://archiveofourown.org/works/47960101']\n",
      "https://archiveofourown.org/works/47917651\n",
      "Empty DataFrame\n",
      "Columns: [fanfic_id, update_date, words, chapter, chapter_max, rating, date_scraped]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "get_up(tv_update_links)\n",
    "get_up(book_update_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_user_engagement(update_links):\n",
    "    user_engagement = pd.DataFrame(columns=['fanfic_id','kudos', 'bookmarks', 'comments', 'hits'])\n",
    "    date_now = datetime.now()\n",
    "    for link in update_links:\n",
    "        try:\n",
    "            newlink=link+'?view_adult=true'\n",
    "            story = requests.get(newlink, headers={\n",
    "                          'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64)'}).text\n",
    "        except requests.exceptions.RequestException:\n",
    "            print(f\"fanfic {link} is taking too long to access.\")\n",
    "            continue\n",
    "        story = BeautifulSoup(story,'html.parser')\n",
    "        fanfic_id = link.replace('https://archiveofourown.org/works/','')\n",
    "        user_engagement = get_user_engagement(story, fanfic_id, user_engagement)\n",
    "        time.sleep(10)\n",
    "    date_now = date_now.strftime(\"%m-%d-%Y\")\n",
    "    user_engagement['date_scraped'] = date_now\n",
    "    conn = sqlite3.connect('output/ao3.db')\n",
    "    querry = \"SELECT * FROM user_engagement\"\n",
    "    user_engagement=user_engagement.append(pd.read_sql_query(querry, conn), ignore_index=True)\n",
    "    user_engagement = user_engagement.drop_duplicates()\n",
    "    user_engagement.to_sql('user_engagement', conn, if_exists='replace', index=False)\n",
    "    conn.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('output/ao3.db')\n",
    "querry = \"SELECT link FROM fanfic\"\n",
    "link_list = pd.read_sql_query(querry, conn)\n",
    "update_user_engagement(link_list.link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2031\n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect('output/ao3.db')\n",
    "querry = \"SELECT link FROM fanfic\"\n",
    "print(len(pd.read_sql_query(querry, conn)))\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimal_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
